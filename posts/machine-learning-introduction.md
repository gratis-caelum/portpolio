# ë¨¸ì‹ ëŸ¬ë‹ ì…ë¬¸: ì²« ë²ˆì§¸ ëª¨ë¸ ë§Œë“¤ê¸°

ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ì˜ˆì¸¡ì´ë‚˜ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤. ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” Pythonì˜ `scikit-learn` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì²« ë²ˆì§¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.

## ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë³¸ ê°œë…

### ì§€ë„í•™ìŠµ vs ë¹„ì§€ë„í•™ìŠµ

- **ì§€ë„í•™ìŠµ (Supervised Learning)**: ì…ë ¥ê³¼ ì •ë‹µì´ ìˆëŠ” ë°ì´í„°ë¡œ í•™ìŠµ
  - ë¶„ë¥˜ (Classification): ì¹´í…Œê³ ë¦¬ ì˜ˆì¸¡
  - íšŒê·€ (Regression): ì—°ì†ì ì¸ ê°’ ì˜ˆì¸¡

- **ë¹„ì§€ë„í•™ìŠµ (Unsupervised Learning)**: ì •ë‹µ ì—†ëŠ” ë°ì´í„°ì—ì„œ íŒ¨í„´ ë°œê²¬
  - êµ°ì§‘í™” (Clustering)
  - ì°¨ì› ì¶•ì†Œ (Dimensionality Reduction)

## í™˜ê²½ ì„¤ì •

í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ë´…ì‹œë‹¤:

```bash
pip install scikit-learn pandas numpy matplotlib seaborn
```

## ì²« ë²ˆì§¸ ë¶„ë¥˜ ëª¨ë¸: ë¶“ê½ƒ ë¶„ë¥˜

ê°€ì¥ ìœ ëª…í•œ ë¨¸ì‹ ëŸ¬ë‹ ì˜ˆì œì¸ ë¶“ê½ƒ(Iris) ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•´ë³´ê² ìŠµë‹ˆë‹¤.

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ë°ì´í„° ë¡œë“œ
iris = datasets.load_iris()
X = iris.data  # íŠ¹ì„± (ê½ƒë°›ì¹¨ ê¸¸ì´, ë„ˆë¹„, ê½ƒì ê¸¸ì´, ë„ˆë¹„)
y = iris.target  # íƒ€ê²Ÿ (ë¶“ê½ƒ ì¢…ë¥˜)

print("íŠ¹ì„± ì´ë¦„:", iris.feature_names)
print("í´ë˜ìŠ¤ ì´ë¦„:", iris.target_names)
print("ë°ì´í„° shape:", X.shape)
```

### ë°ì´í„° íƒìƒ‰

```python
# ë°ì´í„°í”„ë ˆì„ ìƒì„±
df = pd.DataFrame(X, columns=iris.feature_names)
df['target'] = y
df['species'] = df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})

print(df.head())
print(df.describe())

# í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
print("í´ë˜ìŠ¤ ë¶„í¬:")
print(df['species'].value_counts())
```

### ë°ì´í„° ì‹œê°í™”

```python
# íŠ¹ì„± ê°„ ê´€ê³„ ì‹œê°í™”
plt.figure(figsize=(12, 8))
sns.pairplot(df, hue='species', diag_kind='hist')
plt.show()

# ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
plt.figure(figsize=(8, 6))
correlation_matrix = df.iloc[:, :-2].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Feature Correlation Matrix')
plt.show()
```

## ëª¨ë¸ í•™ìŠµ ë° í‰ê°€

### ë°ì´í„° ë¶„í• 

```python
# í•™ìŠµìš©ê³¼ í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¶„í•  (80:20)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"í•™ìŠµ ë°ì´í„°: {X_train.shape}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")
```

### ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸

```python
# ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train, y_train)

# ì˜ˆì¸¡
y_pred_log = log_reg.predict(X_test)

# ì •í™•ë„ ê³„ì‚°
accuracy_log = accuracy_score(y_test, y_pred_log)
print(f"ë¡œì§€ìŠ¤í‹± íšŒê·€ ì •í™•ë„: {accuracy_log:.3f}")
```

### ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸

```python
# ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# ì˜ˆì¸¡
y_pred_rf = rf_model.predict(X_test)

# ì •í™•ë„ ê³„ì‚°
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f"ëœë¤ í¬ë ˆìŠ¤íŠ¸ ì •í™•ë„: {accuracy_rf:.3f}")
```

## ëª¨ë¸ í‰ê°€

### í˜¼ë™ í–‰ë ¬ (Confusion Matrix)

```python
# ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ì˜ í˜¼ë™ í–‰ë ¬
cm = confusion_matrix(y_test, y_pred_rf)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=iris.target_names,
            yticklabels=iris.target_names)
plt.title('Confusion Matrix - Random Forest')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()
```

### ë¶„ë¥˜ ë¦¬í¬íŠ¸

```python
# ìƒì„¸í•œ ë¶„ë¥˜ ë¦¬í¬íŠ¸
print("ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
print(classification_report(y_test, y_pred_rf, target_names=iris.target_names))
```

### íŠ¹ì„± ì¤‘ìš”ë„

```python
# íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
feature_importance = rf_model.feature_importances_
feature_names = iris.feature_names

plt.figure(figsize=(10, 6))
indices = np.argsort(feature_importance)[::-1]
plt.bar(range(len(feature_importance)), feature_importance[indices])
plt.xticks(range(len(feature_importance)), 
           [feature_names[i] for i in indices], rotation=45)
plt.title('Feature Importance - Random Forest')
plt.tight_layout()
plt.show()
```

## ìƒˆë¡œìš´ ë°ì´í„° ì˜ˆì¸¡

```python
# ìƒˆë¡œìš´ ë°ì´í„° í¬ì¸íŠ¸ë¡œ ì˜ˆì¸¡í•´ë³´ê¸°
new_sample = np.array([[5.1, 3.5, 1.4, 0.2]])  # ìƒˆë¡œìš´ ë¶“ê½ƒ ì¸¡ì •ê°’

prediction = rf_model.predict(new_sample)
prediction_proba = rf_model.predict_proba(new_sample)

print(f"ì˜ˆì¸¡ ê²°ê³¼: {iris.target_names[prediction[0]]}")
print("ì˜ˆì¸¡ í™•ë¥ :")
for i, prob in enumerate(prediction_proba[0]):
    print(f"  {iris.target_names[i]}: {prob:.3f}")
```

## ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ íŒ

### 1. êµì°¨ ê²€ì¦ (Cross Validation)

```python
from sklearn.model_selection import cross_val_score

# 5-fold êµì°¨ ê²€ì¦
cv_scores = cross_val_score(rf_model, X, y, cv=5)
print(f"êµì°¨ ê²€ì¦ í‰ê·  ì •í™•ë„: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")
```

### 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

```python
from sklearn.model_selection import GridSearchCV

# ê·¸ë¦¬ë“œ ì„œì¹˜ë¡œ ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸°
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 7, None],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy'
)

grid_search.fit(X_train, y_train)
print(f"ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
print(f"ìµœê³  êµì°¨ ê²€ì¦ ì ìˆ˜: {grid_search.best_score_:.3f}")
```

## ë§ˆë¬´ë¦¬

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” scikit-learnì„ ì‚¬ìš©í•˜ì—¬ ì²« ë²ˆì§¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ë§Œë“¤ì–´ë´¤ìŠµë‹ˆë‹¤. ì£¼ìš” ë‚´ìš©ì„ ì •ë¦¬í•˜ë©´:

1. **ë°ì´í„° íƒìƒ‰**: ë°ì´í„°ì˜ êµ¬ì¡°ì™€ ë¶„í¬ íŒŒì•…
2. **ëª¨ë¸ í•™ìŠµ**: ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ëª¨ë¸ í›ˆë ¨
3. **ëª¨ë¸ í‰ê°€**: ì •í™•ë„, í˜¼ë™ í–‰ë ¬, ë¶„ë¥˜ ë¦¬í¬íŠ¸ í™œìš©
4. **ì˜ˆì¸¡**: ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰

### ë‹¤ìŒ ë‹¨ê³„

- ë” ë³µì¡í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ ì—°ìŠµí•˜ê¸°
- ë‹¤ì–‘í•œ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ì‹œë„í•˜ê¸°
- íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ê¸°ë²• í•™ìŠµí•˜ê¸°
- ëª¨ë¸ í•´ì„ ê¸°ë²• ìµíˆê¸°

ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ì—°ë½ ì£¼ì„¸ìš”! ğŸ¤–

### ì°¸ê³  ìë£Œ

- [Scikit-learn Documentation](https://scikit-learn.org/stable/)
- [Machine Learning Mastery](https://machinelearningmastery.com/)
- [Kaggle Learn](https://www.kaggle.com/learn)